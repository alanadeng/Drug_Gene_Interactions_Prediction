{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from node2vec import Node2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "import graph_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading Data, Generate Graph & Embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "data = ['preprocessed_34_10.tsv', 'preprocessed_42_10.tsv']\n",
    "\n",
    "file_path = \"../data/\" + data[0]\n",
    "df = pd.read_csv(file_path, sep='\\t', index_col=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "G = graph_prep.nx_drug_gene_bipartite(df)\n",
    "#node2vec_embeddings = graph_prep.node2vec_embedding(G)\n",
    "#deepwalk_embeddings = graph_prep.deepwalk_embedding(G)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Node2Vec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "Computing transition probabilities:   0%|          | 0/1637 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e5ec98ff9f6d4d8aa793db7c5601ee85"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 4): 100%|██████████| 75/75 [00:14<00:00,  5.29it/s]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "node2vec_embeddings = graph_prep.node2vec_embedding(G)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Deep Walk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "Computing transition probabilities:   0%|          | 0/1637 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "986f274f2e7f4a1e8fb645a23ef94025"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating walks (CPU: 4): 100%|██████████| 75/75 [00:13<00:00,  5.45it/s]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m deepwalk_embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mgraph_prep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdeepwalk_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mG\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/drug_gene_interaction_prediction/code/graph_prep.py:78\u001B[0m, in \u001B[0;36mdeepwalk_embedding\u001B[0;34m(G, dimensions, walk_length, num_walks, workers, window, min_count, batch_words)\u001B[0m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeepwalk_embedding\u001B[39m(G, dimensions\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m128\u001B[39m, walk_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m15\u001B[39m, num_walks\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m300\u001B[39m, workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, window\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, min_count\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, batch_words\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m):\n\u001B[1;32m     77\u001B[0m     node2vec \u001B[38;5;241m=\u001B[39m Node2Vec(G, dimensions\u001B[38;5;241m=\u001B[39mdimensions, walk_length\u001B[38;5;241m=\u001B[39mwalk_length, num_walks\u001B[38;5;241m=\u001B[39mnum_walks, workers\u001B[38;5;241m=\u001B[39mworkers, p\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, q\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 78\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mnode2vec\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwindow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_count\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmin_count\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_words\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_words\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;66;03m# Get node embeddings\u001B[39;00m\n\u001B[1;32m     81\u001B[0m     node_embeddings \u001B[38;5;241m=\u001B[39m {node: model\u001B[38;5;241m.\u001B[39mwv[node] \u001B[38;5;28;01mfor\u001B[39;00m node \u001B[38;5;129;01min\u001B[39;00m G\u001B[38;5;241m.\u001B[39mnodes()}\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mlp/lib/python3.8/site-packages/node2vec/node2vec.py:199\u001B[0m, in \u001B[0;36mNode2Vec.fit\u001B[0;34m(self, **skip_gram_params)\u001B[0m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msg\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m skip_gram_params:\n\u001B[1;32m    197\u001B[0m     skip_gram_params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msg\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 199\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgensim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mWord2Vec\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwalks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mskip_gram_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mlp/lib/python3.8/site-packages/gensim/models/word2vec.py:430\u001B[0m, in \u001B[0;36mWord2Vec.__init__\u001B[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001B[0m\n\u001B[1;32m    428\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_corpus_sanity(corpus_iterable\u001B[38;5;241m=\u001B[39mcorpus_iterable, corpus_file\u001B[38;5;241m=\u001B[39mcorpus_file, passes\u001B[38;5;241m=\u001B[39m(epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m    429\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuild_vocab(corpus_iterable\u001B[38;5;241m=\u001B[39mcorpus_iterable, corpus_file\u001B[38;5;241m=\u001B[39mcorpus_file, trim_rule\u001B[38;5;241m=\u001B[39mtrim_rule)\n\u001B[0;32m--> 430\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    431\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcorpus_iterable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcorpus_iterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcorpus_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcorpus_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtotal_examples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcorpus_count\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    432\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_words\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcorpus_total_words\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_alpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    433\u001B[0m \u001B[43m        \u001B[49m\u001B[43mend_alpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmin_alpha\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompute_loss\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    434\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    435\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trim_rule \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mlp/lib/python3.8/site-packages/gensim/models/word2vec.py:1073\u001B[0m, in \u001B[0;36mWord2Vec.train\u001B[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m   1070\u001B[0m     callback\u001B[38;5;241m.\u001B[39mon_epoch_begin(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1072\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m corpus_iterable \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1073\u001B[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_epoch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1074\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcorpus_iterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcur_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcur_epoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtotal_examples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_examples\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1075\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_words\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_words\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mqueue_factor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mqueue_factor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreport_delay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreport_delay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1076\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1077\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1078\u001B[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_epoch_corpusfile(\n\u001B[1;32m   1079\u001B[0m         corpus_file, cur_epoch\u001B[38;5;241m=\u001B[39mcur_epoch, total_examples\u001B[38;5;241m=\u001B[39mtotal_examples, total_words\u001B[38;5;241m=\u001B[39mtotal_words,\n\u001B[1;32m   1080\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mlp/lib/python3.8/site-packages/gensim/models/word2vec.py:1434\u001B[0m, in \u001B[0;36mWord2Vec._train_epoch\u001B[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001B[0m\n\u001B[1;32m   1431\u001B[0m     thread\u001B[38;5;241m.\u001B[39mdaemon \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m  \u001B[38;5;66;03m# make interrupting the process with ctrl+c easier\u001B[39;00m\n\u001B[1;32m   1432\u001B[0m     thread\u001B[38;5;241m.\u001B[39mstart()\n\u001B[0;32m-> 1434\u001B[0m trained_word_count, raw_word_count, job_tally \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_log_epoch_progress\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1435\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprogress_queue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjob_queue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcur_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcur_epoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtotal_examples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_examples\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1436\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtotal_words\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_words\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreport_delay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreport_delay\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_corpus_file_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1437\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1439\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m trained_word_count, raw_word_count, job_tally\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mlp/lib/python3.8/site-packages/gensim/models/word2vec.py:1289\u001B[0m, in \u001B[0;36mWord2Vec._log_epoch_progress\u001B[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001B[0m\n\u001B[1;32m   1286\u001B[0m unfinished_worker_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mworkers\n\u001B[1;32m   1288\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m unfinished_worker_count \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 1289\u001B[0m     report \u001B[38;5;241m=\u001B[39m \u001B[43mprogress_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# blocks if workers too slow\u001B[39;00m\n\u001B[1;32m   1290\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m report \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:  \u001B[38;5;66;03m# a thread reporting that it finished\u001B[39;00m\n\u001B[1;32m   1291\u001B[0m         unfinished_worker_count \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mlp/lib/python3.8/queue.py:170\u001B[0m, in \u001B[0;36mQueue.get\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_qsize():\n\u001B[0;32m--> 170\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnot_empty\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m timeout \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    172\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m must be a non-negative number\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mlp/lib/python3.8/threading.py:302\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[1;32m    301\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 302\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    303\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "deepwalk_embeddings = graph_prep.deepwalk_embedding(G)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Node2Vec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, precision_recall_curve, auc\n",
    "from scipy.stats import loguniform, randint\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Directory for saving results\n",
    "save_dir = '../res/node2vec'\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store metrics and models\n",
    "metrics = {}\n",
    "models = {}\n",
    "roc_data = {}\n",
    "prc_data = {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hyperparameter distributions\n",
    "lr_param_dist = {'C': loguniform(0.001, 1000)}\n",
    "xgb_param_dist = {\n",
    "    'learning_rate': loguniform(0.01, 0.2),\n",
    "    'n_estimators': randint(50, 1000),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'gamma': loguniform(0.001, 1),\n",
    "    'subsample': loguniform(0.5, 1),\n",
    "    'colsample_bytree': loguniform(0.5, 1)\n",
    "}\n",
    "svm_param_dist = {'C': loguniform(0.001, 1000), 'gamma': loguniform(0.001, 1)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Softmax Layer Neural Network\n",
    "class SoftmaxNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SoftmaxNN, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.softmax(self.fc(x), dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function for hyperparameter tuning\n",
    "def tune_hyperparameters(clf, param_dist, X_train, y_train):\n",
    "    random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=100, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    return random_search.best_estimator_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function for training and evaluating the model\n",
    "def train_evaluate_model(clf, X_train, X_test, y_train, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_probs = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred),\n",
    "        #'AUC-ROC': roc_auc_score(y_test, y_probs),\n",
    "        #'AUC-PRC': auc(*precision_recall_curve(y_test, y_probs)[:2])\n",
    "    }\n",
    "\n",
    "    roc_data = roc_curve(y_test, y_probs)\n",
    "    prc_data = precision_recall_curve(y_test, y_probs)\n",
    "\n",
    "    print(\"Metrics: \\n\")\n",
    "    print(metrics)\n",
    "    print(\"AUC-ROC: \")\n",
    "    print(roc_data)\n",
    "    print(\"AUC-PRC: \")\n",
    "    print(prc_data)\n",
    "\n",
    "    return metrics, roc_data, prc_data\n",
    "\n",
    "\n",
    "# Function for training and evaluating the neural network\n",
    "def train_evaluate_nn(model, X_train, X_test, y_train, y_test, epochs=100, learning_rate=0.001):\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_torch = torch.tensor(X_train.astype(np.float32))\n",
    "    X_test_torch = torch.tensor(X_test.astype(np.float32))\n",
    "    y_train_torch = torch.tensor(y_train.astype(np.int64))\n",
    "    y_test_torch = torch.tensor(y_test.astype(np.int64))\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_torch)\n",
    "        loss = criterion(outputs, y_train_torch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_torch = model(X_test_torch)\n",
    "        y_pred = torch.max(y_pred_torch, 1)[1].numpy()\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function for plotting metrics\n",
    "def plot_metrics(roc_data, prc_data, metrics, title, save_dir):\n",
    "    fpr, tpr, _ = roc_data\n",
    "    precision, recall, _ = prc_data\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # ROC Curve\n",
    "    axs[0].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC AUC = {metrics[\"AUC-ROC\"]:.2f}')\n",
    "    axs[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    axs[0].set_title(f'{title} - ROC Curve')\n",
    "    axs[0].set_xlabel('False Positive Rate')\n",
    "    axs[0].set_ylabel('True Positive Rate')\n",
    "    axs[0].legend(loc='lower right')\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    axs[1].plot(recall, precision, color='darkorange', lw=2, label=f'PRC AUC = {metrics[\"AUC-PRC\"]:.2f}')\n",
    "    axs[1].set_title(f'{title} - Precision-Recall Curve')\n",
    "    axs[1].set_xlabel('Recall')\n",
    "    axs[1].set_ylabel('Precision')\n",
    "    axs[1].legend(loc='lower left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir + '/' + title + '.png')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loop through sampling methods\n",
    "for sampling in ['negative', 'stratified', 'down']:\n",
    "    # Split edges and labels for training and test\n",
    "    edges_train, edges_test, y_train, y_test = graph_prep.edge_train_test_split(df, method=sampling)\n",
    "\n",
    "    # Construct edge features\n",
    "    X_train = np.array([graph_prep.edge_features(edge, node2vec_embeddings) for edge in edges_train])\n",
    "    X_test = np.array([graph_prep.edge_features(edge, node2vec_embeddings) for edge in edges_test])\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    print(\"############# Node2Vec + LR + \" + sampling + \" #############\")\n",
    "    # Train and evaluate Logistic Regression\n",
    "    lr_clf = tune_hyperparameters(LogisticRegression(max_iter=100000), lr_param_dist, X_train, y_train)\n",
    "    lr_metrics, lr_roc_data, lr_prc_data = train_evaluate_model(lr_clf, X_train, X_test, y_train, y_test)\n",
    "    models['logistic_regression_' + sampling] = lr_clf\n",
    "    metrics['logistic_regression_' + sampling] = lr_metrics\n",
    "    roc_data['logistic_regression_' + sampling] = lr_roc_data\n",
    "    prc_data['logistic_regression_' + sampling] = lr_prc_data\n",
    "\n",
    "    # print(\"############# Node2Vec + XGB + \" + sampling + \" #############\\n\")\n",
    "    # # Train and evaluate XGBoost\n",
    "    # xgb_clf = tune_hyperparameters(xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'), xgb_param_dist, X_train, y_train)\n",
    "    # xgb_metrics, xgb_roc_data, xgb_prc_data = train_evaluate_model(xgb_clf, X_train, X_test, y_train, y_test)\n",
    "    # models['xgboost_' + sampling] = xgb_clf\n",
    "    # metrics['xgboost_' + sampling] = xgb_metrics\n",
    "    # roc_data['xgboost_' + sampling] = xgb_roc_data\n",
    "    # prc_data['xgboost_' + sampling] = xgb_prc_data\n",
    "    #\n",
    "    # print(\"############# Node2Vec + SVM + \" + sampling + \" #############\\n\")\n",
    "    # # Train and evaluate SVM\n",
    "    # svm_clf = tune_hyperparameters(SVC(probability=True), svm_param_dist, X_train, y_train)\n",
    "    # svm_metrics, svm_roc_data, svm_prc_data = train_evaluate_model(svm_clf, X_train, X_test, y_train, y_test)\n",
    "    # models['svm_' + sampling] = svm_clf\n",
    "    # metrics['svm_' + sampling] = svm_metrics\n",
    "    # roc_data['svm_' + sampling] = svm_roc_data\n",
    "    # prc_data['svm_' + sampling] = svm_prc_data\n",
    "    #\n",
    "    # print(\"############# Node2Vec + NN + \" + sampling + \" #############\\n\")\n",
    "    # # Train and evaluate Softmax Neural Network\n",
    "    # nn_model = SoftmaxNN(X_train.shape[1])\n",
    "    # nn_metrics = train_evaluate_nn(nn_model, X_train, X_test, y_train, y_test)\n",
    "    # models['softmax_nn_' + sampling] = nn_model\n",
    "    # metrics['softmax_nn_' + sampling] = nn_metrics\n",
    "\n",
    "# Save models and metrics\n",
    "for model_key in models:\n",
    "    pickle.dump(models[model_key], open(f'{save_dir}/{model_key}_model.pkl', 'wb'))\n",
    "    pd.DataFrame([metrics[model_key]]).to_csv(f'{save_dir}/{model_key}_metrics.csv', index=False)\n",
    "\n",
    "# Plotting\n",
    "for model_key in roc_data:\n",
    "    plot_metrics(roc_data[model_key], prc_data[model_key], metrics[model_key], model_key, save_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}