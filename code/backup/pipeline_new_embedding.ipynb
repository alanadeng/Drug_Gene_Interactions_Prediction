{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from node2vec import Node2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "import graph_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loading Data, Generate Graph & Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = ['preprocessed_34_10.tsv', 'preprocessed_42_10.tsv']\n",
    "\n",
    "###### edit here to change dataset ######\n",
    "dat_idx = 0 # choose from [0,1]\n",
    "#########################################\n",
    "\n",
    "file_path = \"../data/\" + data[dat_idx]\n",
    "df = pd.read_csv(file_path, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "G = graph_prep.nx_drug_gene_bipartite(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m/Users/chutongdeng/Documents/GitHub/drug_gene_interaction_prediction/code/pipeline_new_embedding.ipynb Cell 5\u001B[0m line \u001B[0;36m1\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/chutongdeng/Documents/GitHub/drug_gene_interaction_prediction/code/pipeline_new_embedding.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001B[0m     node_embeddings \u001B[39m=\u001B[39m graph_prep\u001B[39m.\u001B[39mdeepwalk_embedding(G)\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/chutongdeng/Documents/GitHub/drug_gene_interaction_prediction/code/pipeline_new_embedding.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001B[0m \u001B[39melif\u001B[39;00m embedding_methods[emd_idx] \u001B[39m==\u001B[39m \u001B[39m'\u001B[39m\u001B[39mbine\u001B[39m\u001B[39m'\u001B[39m:\n\u001B[0;32m---> <a href='vscode-notebook-cell:/Users/chutongdeng/Documents/GitHub/drug_gene_interaction_prediction/code/pipeline_new_embedding.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001B[0m     node_embeddings \u001B[39m=\u001B[39m graph_prep\u001B[39m.\u001B[39;49mbine_embedding(G)\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/chutongdeng/Documents/GitHub/drug_gene_interaction_prediction/code/pipeline_new_embedding.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/chutongdeng/Documents/GitHub/drug_gene_interaction_prediction/code/pipeline_new_embedding.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mValueError\u001B[39;00m(\u001B[39m\"\u001B[39m\u001B[39mInvalid embedding methods.\u001B[39m\u001B[39m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/GitHub/drug_gene_interaction_prediction/code/graph_prep.py:142\u001B[0m, in \u001B[0;36mbine_embedding\u001B[0;34m(G, dimensions, walk_length, num_walks, window, workers)\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mbine_embedding\u001B[39m(G, dimensions\u001B[39m=\u001B[39m\u001B[39m128\u001B[39m, walk_length\u001B[39m=\u001B[39m\u001B[39m15\u001B[39m, num_walks\u001B[39m=\u001B[39m\u001B[39m300\u001B[39m, window\u001B[39m=\u001B[39m\u001B[39m10\u001B[39m, workers\u001B[39m=\u001B[39m\u001B[39m4\u001B[39m):\n\u001B[1;32m    141\u001B[0m     walks \u001B[39m=\u001B[39m biased_random_walks(G, num_walks, walk_length)\n\u001B[0;32m--> 142\u001B[0m     model \u001B[39m=\u001B[39m Word2Vec(walks, vector_size\u001B[39m=\u001B[39;49mdimensions, window\u001B[39m=\u001B[39;49mwindow, workers\u001B[39m=\u001B[39;49mworkers, sg\u001B[39m=\u001B[39;49m\u001B[39m1\u001B[39;49m)\n\u001B[1;32m    143\u001B[0m     node_embeddings \u001B[39m=\u001B[39m {node: model\u001B[39m.\u001B[39mwv[node] \u001B[39mfor\u001B[39;00m node \u001B[39min\u001B[39;00m G\u001B[39m.\u001B[39mnodes()}\n\u001B[1;32m    144\u001B[0m     \u001B[39mreturn\u001B[39;00m node_embeddings\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mlp/lib/python3.8/site-packages/gensim/models/word2vec.py:430\u001B[0m, in \u001B[0;36mWord2Vec.__init__\u001B[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001B[0m\n\u001B[1;32m    428\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_check_corpus_sanity(corpus_iterable\u001B[39m=\u001B[39mcorpus_iterable, corpus_file\u001B[39m=\u001B[39mcorpus_file, passes\u001B[39m=\u001B[39m(epochs \u001B[39m+\u001B[39m \u001B[39m1\u001B[39m))\n\u001B[1;32m    429\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mbuild_vocab(corpus_iterable\u001B[39m=\u001B[39mcorpus_iterable, corpus_file\u001B[39m=\u001B[39mcorpus_file, trim_rule\u001B[39m=\u001B[39mtrim_rule)\n\u001B[0;32m--> 430\u001B[0m     \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mtrain(\n\u001B[1;32m    431\u001B[0m         corpus_iterable\u001B[39m=\u001B[39;49mcorpus_iterable, corpus_file\u001B[39m=\u001B[39;49mcorpus_file, total_examples\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mcorpus_count,\n\u001B[1;32m    432\u001B[0m         total_words\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mcorpus_total_words, epochs\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mepochs, start_alpha\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49malpha,\n\u001B[1;32m    433\u001B[0m         end_alpha\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mmin_alpha, compute_loss\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mcompute_loss, callbacks\u001B[39m=\u001B[39;49mcallbacks)\n\u001B[1;32m    434\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m    435\u001B[0m     \u001B[39mif\u001B[39;00m trim_rule \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mlp/lib/python3.8/site-packages/gensim/models/word2vec.py:1073\u001B[0m, in \u001B[0;36mWord2Vec.train\u001B[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m   1070\u001B[0m     callback\u001B[39m.\u001B[39mon_epoch_begin(\u001B[39mself\u001B[39m)\n\u001B[1;32m   1072\u001B[0m \u001B[39mif\u001B[39;00m corpus_iterable \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[0;32m-> 1073\u001B[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_train_epoch(\n\u001B[1;32m   1074\u001B[0m         corpus_iterable, cur_epoch\u001B[39m=\u001B[39;49mcur_epoch, total_examples\u001B[39m=\u001B[39;49mtotal_examples,\n\u001B[1;32m   1075\u001B[0m         total_words\u001B[39m=\u001B[39;49mtotal_words, queue_factor\u001B[39m=\u001B[39;49mqueue_factor, report_delay\u001B[39m=\u001B[39;49mreport_delay,\n\u001B[1;32m   1076\u001B[0m         callbacks\u001B[39m=\u001B[39;49mcallbacks, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1077\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m   1078\u001B[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_train_epoch_corpusfile(\n\u001B[1;32m   1079\u001B[0m         corpus_file, cur_epoch\u001B[39m=\u001B[39mcur_epoch, total_examples\u001B[39m=\u001B[39mtotal_examples, total_words\u001B[39m=\u001B[39mtotal_words,\n\u001B[1;32m   1080\u001B[0m         callbacks\u001B[39m=\u001B[39mcallbacks, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mlp/lib/python3.8/site-packages/gensim/models/word2vec.py:1434\u001B[0m, in \u001B[0;36mWord2Vec._train_epoch\u001B[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001B[0m\n\u001B[1;32m   1431\u001B[0m     thread\u001B[39m.\u001B[39mdaemon \u001B[39m=\u001B[39m \u001B[39mTrue\u001B[39;00m  \u001B[39m# make interrupting the process with ctrl+c easier\u001B[39;00m\n\u001B[1;32m   1432\u001B[0m     thread\u001B[39m.\u001B[39mstart()\n\u001B[0;32m-> 1434\u001B[0m trained_word_count, raw_word_count, job_tally \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_log_epoch_progress(\n\u001B[1;32m   1435\u001B[0m     progress_queue, job_queue, cur_epoch\u001B[39m=\u001B[39;49mcur_epoch, total_examples\u001B[39m=\u001B[39;49mtotal_examples,\n\u001B[1;32m   1436\u001B[0m     total_words\u001B[39m=\u001B[39;49mtotal_words, report_delay\u001B[39m=\u001B[39;49mreport_delay, is_corpus_file_mode\u001B[39m=\u001B[39;49m\u001B[39mFalse\u001B[39;49;00m,\n\u001B[1;32m   1437\u001B[0m )\n\u001B[1;32m   1439\u001B[0m \u001B[39mreturn\u001B[39;00m trained_word_count, raw_word_count, job_tally\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mlp/lib/python3.8/site-packages/gensim/models/word2vec.py:1289\u001B[0m, in \u001B[0;36mWord2Vec._log_epoch_progress\u001B[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001B[0m\n\u001B[1;32m   1286\u001B[0m unfinished_worker_count \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mworkers\n\u001B[1;32m   1288\u001B[0m \u001B[39mwhile\u001B[39;00m unfinished_worker_count \u001B[39m>\u001B[39m \u001B[39m0\u001B[39m:\n\u001B[0;32m-> 1289\u001B[0m     report \u001B[39m=\u001B[39m progress_queue\u001B[39m.\u001B[39;49mget()  \u001B[39m# blocks if workers too slow\u001B[39;00m\n\u001B[1;32m   1290\u001B[0m     \u001B[39mif\u001B[39;00m report \u001B[39mis\u001B[39;00m \u001B[39mNone\u001B[39;00m:  \u001B[39m# a thread reporting that it finished\u001B[39;00m\n\u001B[1;32m   1291\u001B[0m         unfinished_worker_count \u001B[39m-\u001B[39m\u001B[39m=\u001B[39m \u001B[39m1\u001B[39m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mlp/lib/python3.8/queue.py:170\u001B[0m, in \u001B[0;36mQueue.get\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[39melif\u001B[39;00m timeout \u001B[39mis\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[1;32m    169\u001B[0m     \u001B[39mwhile\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_qsize():\n\u001B[0;32m--> 170\u001B[0m         \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mnot_empty\u001B[39m.\u001B[39;49mwait()\n\u001B[1;32m    171\u001B[0m \u001B[39melif\u001B[39;00m timeout \u001B[39m<\u001B[39m \u001B[39m0\u001B[39m:\n\u001B[1;32m    172\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mValueError\u001B[39;00m(\u001B[39m\"\u001B[39m\u001B[39m'\u001B[39m\u001B[39mtimeout\u001B[39m\u001B[39m'\u001B[39m\u001B[39m must be a non-negative number\u001B[39m\u001B[39m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mlp/lib/python3.8/threading.py:302\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    300\u001B[0m \u001B[39mtry\u001B[39;00m:    \u001B[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[1;32m    301\u001B[0m     \u001B[39mif\u001B[39;00m timeout \u001B[39mis\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[0;32m--> 302\u001B[0m         waiter\u001B[39m.\u001B[39;49macquire()\n\u001B[1;32m    303\u001B[0m         gotit \u001B[39m=\u001B[39m \u001B[39mTrue\u001B[39;00m\n\u001B[1;32m    304\u001B[0m     \u001B[39melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "embedding_methods = ['node2vec', 'deepwalk', 'bine']\n",
    "\n",
    "###### edit here to change the node embedding method ######\n",
    "emd_idx = 2 # choose from [0,1]\n",
    "#########################################\n",
    "\n",
    "if embedding_methods[emd_idx] == 'node2vec':\n",
    "    node_embeddings = graph_prep.node2vec_embedding(G)\n",
    "elif embedding_methods[emd_idx] == 'deepwalk':\n",
    "    node_embeddings = graph_prep.deepwalk_embedding(G)\n",
    "elif embedding_methods[emd_idx] == 'bine':\n",
    "    node_embeddings = graph_prep.bine_embedding(G)\n",
    "else:\n",
    "    raise ValueError(\"Invalid embedding methods.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model Functions & Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, precision_recall_curve, auc\n",
    "from scipy.stats import loguniform, randint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter distributions\n",
    "lr_param_dist = {'C': loguniform(0.001, 1000)}\n",
    "xgb_param_dist = {\n",
    "    'learning_rate': loguniform(0.01, 0.2),\n",
    "    'n_estimators': randint(50, 1000),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'gamma': loguniform(0.001, 1),\n",
    "    'subsample': loguniform(0.5, 1),\n",
    "    'colsample_bytree': loguniform(0.5, 1)\n",
    "}\n",
    "svm_param_dist = {'C': loguniform(0.001, 1000), 'gamma': loguniform(0.001, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "\n",
    "learning_rate_range = np.logspace(-4, -1, 10)\n",
    "epochs_range = range(50, 300, 50)\n",
    "\n",
    "\n",
    "def tune_nn_hyperparameters(model_class, X, y, learning_rate_range, epochs_range, n_splits=5, n_iter=100):\n",
    "    best_score = 0\n",
    "    best_params = {'learning_rate': None, 'epochs': None}\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        lr = random.choice(learning_rate_range)\n",
    "        epochs = random.choice(epochs_range)\n",
    "\n",
    "        scores = []\n",
    "        for train_index, val_index in kfold.split(X):\n",
    "            X_train, X_val = X[train_index], X[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            model = model_class(X_train.shape[1])\n",
    "            metrics, _, _ = train_evaluate_nn(model, X_train, X_val, y_train, y_val, epochs=epochs, learning_rate=lr)\n",
    "            score = metrics['AUC-ROC']  # Choose your metric here\n",
    "            scores.append(score)\n",
    "\n",
    "        average_score = np.mean(scores)\n",
    "        if average_score > best_score:\n",
    "            best_score = average_score\n",
    "            best_params['learning_rate'] = lr\n",
    "            best_params['epochs'] = epochs\n",
    "\n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Softmax Layer Neural Network\n",
    "class SoftmaxNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SoftmaxNN, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.softmax(self.fc(x), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Function for hyperparameter tuning\n",
    "def tune_hyperparameters(clf, param_dist, X_train, y_train):\n",
    "    #random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=100, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "    random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=100, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    return random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Function for training and evaluating the model\n",
    "def train_evaluate_model(clf, X_train, X_test, y_train, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_probs = clf.predict_proba(X_test)[:, 1]\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
    "\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred),\n",
    "        'AUC-ROC': roc_auc_score(y_test, y_probs),\n",
    "        'AUC-PRC': auc(recall, precision)\n",
    "    }\n",
    "\n",
    "    roc_data = roc_curve(y_test, y_probs)\n",
    "    prc_data = precision_recall_curve(y_test, y_probs)\n",
    "\n",
    "    print(\"Metrics: \", end='')\n",
    "    print(metrics)\n",
    "    # print(\"AUC-ROC: \")\n",
    "    # print(roc_data)\n",
    "    # print(\"AUC-PRC: \")\n",
    "    # print(prc_data)\n",
    "\n",
    "    return metrics, roc_data, prc_data\n",
    "\n",
    "\n",
    "def train_evaluate_nn(model, X_train, X_test, y_train, y_test, epochs=200, learning_rate=0.01):\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_torch = torch.tensor(X_train.astype(np.float32))\n",
    "    X_test_torch = torch.tensor(X_test.astype(np.float32))\n",
    "    y_train_torch = torch.tensor(y_train.astype(np.int64))\n",
    "    y_test_torch = torch.tensor(y_test.astype(np.int64))\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_torch)\n",
    "        loss = criterion(outputs, y_train_torch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_probs_torch = model(X_test_torch)\n",
    "        y_probs = y_probs_torch.numpy()[:, 1]  # Probabilities for the positive class\n",
    "        y_pred = np.argmax(y_probs_torch.numpy(), axis=1)\n",
    "\n",
    "    # Check if y_probs is valid\n",
    "    if not (0 <= y_probs).all() and (y_probs <= 1).all():\n",
    "        raise ValueError(\"y_probs contains values outside the range [0, 1]\")\n",
    "\n",
    "    # Check if y_test is in the correct format\n",
    "    if not set(np.unique(y_test)).issubset({0, 1}):\n",
    "        raise ValueError(\"y_test should only contain 0 and 1\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        roc_data = roc_curve(y_test, y_probs)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
    "        metrics['AUC-ROC'] = roc_auc_score(y_test, y_probs)\n",
    "        metrics['AUC-PRC'] = auc(recall, precision)\n",
    "        prc_data = (precision, recall, _)\n",
    "        #print(metrics)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error calculating ROC or PRC: {e}\")\n",
    "        roc_data = ([], [], [])\n",
    "        prc_data = ([], [], [])\n",
    "\n",
    "    return metrics, roc_data, prc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_metrics(roc_data, prc_data, save_fig=False, filename='model_evaluation_plots.png'):\n",
    "    # Unpack ROC data\n",
    "    fpr, tpr, _ = roc_data\n",
    "    # Unpack Precision-Recall data\n",
    "    precision, recall, _ = prc_data\n",
    "\n",
    "    # Create subplots\n",
    "    fig, (ax_roc, ax_prc) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Plot ROC Curve\n",
    "    ax_roc.plot(fpr, tpr, color='blue', lw=2, label='ROC curve')\n",
    "    ax_roc.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "    ax_roc.set_xlim([0.0, 1.0])\n",
    "    ax_roc.set_ylim([0.0, 1.05])\n",
    "    ax_roc.set_xlabel('False Positive Rate')\n",
    "    ax_roc.set_ylabel('True Positive Rate')\n",
    "    ax_roc.set_title('ROC Curve')\n",
    "    ax_roc.legend(loc=\"lower right\")\n",
    "\n",
    "    # Plot Precision-Recall Curve\n",
    "    ax_prc.plot(recall, precision, color='green', lw=2, label='PR curve')\n",
    "    ax_prc.set_xlim([0.0, 1.0])\n",
    "    ax_prc.set_ylim([0.0, 1.05])\n",
    "    ax_prc.set_xlabel('Recall')\n",
    "    ax_prc.set_ylabel('Precision')\n",
    "    ax_prc.set_title('Precision-Recall Curve')\n",
    "    ax_prc.legend(loc=\"lower left\")\n",
    "\n",
    "    # Show the plots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    if save_fig:\n",
    "        plt.savefig(filename, dpi=300)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Sampling, Train-Test-Split, Dimensionality Reduction, Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "classification_models = ['lr', 'svm', 'xgb', 'softmax']\n",
    "\n",
    "###### edit here to change classification model ######\n",
    "clf_idx = 0 # choose from [0,3]\n",
    "#########################################\n",
    "\n",
    "\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(classification_models[clf_idx])\n",
    "for sampling in ['random_under', 'distance_under']:\n",
    "#for sampling in ['distance_under']:\n",
    "    for if_stratified in [True, False]:\n",
    "    #for if_stratified in [False]:\n",
    "        for if_PCA in [True, False]:\n",
    "        #for if_PCA in [True]:\n",
    "\n",
    "\n",
    "            print(\"----------------------------\")\n",
    "\n",
    "            # sampling\n",
    "            sampled_edges, sampled_labels = graph_prep.negative_sampling(df, node_embeddings, method=sampling)\n",
    "            print(\"Sampling Method: \", sampling)\n",
    "\n",
    "            if if_PCA:\n",
    "                # dimensionality reduction\n",
    "                print(\"PCA applied. \", end='')\n",
    "                sampled_edges = graph_prep.perform_pca(sampled_edges)\n",
    "            else:\n",
    "                print(\"PCA not applied.\")\n",
    "\n",
    "            # train-test split\n",
    "            X_train, X_test, y_train, y_test = graph_prep.edge_train_test_split(sampled_edges, sampled_labels, stratified=if_stratified)\n",
    "            print(\"Stratified Train-Test Split: \", if_stratified)\n",
    "\n",
    "            # LR\n",
    "            if classification_models[clf_idx] == 'lr':\n",
    "                lr_clf = tune_hyperparameters(LogisticRegression(max_iter=100000), lr_param_dist, X_train, y_train)\n",
    "                metrics, roc_data, prc_data = train_evaluate_model(lr_clf, X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "            # SVM\n",
    "            elif classification_models[clf_idx] == 'svm':\n",
    "                svm_clf = tune_hyperparameters(SVC(probability=True), svm_param_dist, X_train, y_train)\n",
    "                metrics, roc_data, prc_data = train_evaluate_model(svm_clf, X_train, X_test, y_train, y_test)\n",
    "\n",
    "            # XGB\n",
    "            elif classification_models[clf_idx] == 'xgb':\n",
    "                xgb_clf = tune_hyperparameters(xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'), xgb_param_dist, X_train, y_train)\n",
    "                metrics, roc_data, prc_data = train_evaluate_model(xgb_clf, X_train, X_test, y_train, y_test)\n",
    "\n",
    "            # Softmax\n",
    "            elif classification_models[clf_idx] == 'softmax':\n",
    "                best_params, best_score = tune_nn_hyperparameters(SoftmaxNN, X_train, y_train, learning_rate_range, epochs_range)\n",
    "                nn_model = SoftmaxNN(X_train.shape[1])\n",
    "                best_learning_rate = best_params['learning_rate']\n",
    "                best_epochs = best_params['epochs']\n",
    "                metrics, roc_data, prc_data = train_evaluate_nn(nn_model, X_train, X_test, y_train, y_test, epochs=best_epochs, learning_rate=best_learning_rate)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Invalid value for classification model.\")\n",
    "\n",
    "\n",
    "            ##### Directory for saving results ######\n",
    "            #save_dir = '../res/'+ classification_models[clf_idx]\n",
    "            save_dir = '../res/'+ classification_models[clf_idx] + '/' + embedding_methods[emd_idx]\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            # Convert the metrics to a pandas DataFrame\n",
    "            metrics_df = pd.DataFrame([metrics])\n",
    "            roc_df = pd.DataFrame ({'fpr':roc_data[0], 'tpr':roc_data[1], '_':roc_data[2]})\n",
    "            prc_df = pd.DataFrame({'precision':prc_data[0],'recall':prc_data[1],'_':np.append(prc_data[2],0)})\n",
    "\n",
    "            #save the results\n",
    "            stratified = \"stratified\" if if_stratified else \"unstratified\"\n",
    "            pca = \"pca\" if if_PCA else \"\"\n",
    "            save_name = save_dir + '/' + sampling +'_' + pca + '_' + stratified\n",
    "            #export to csv\n",
    "            metrics_df.to_csv(save_name+'.csv', index=False)\n",
    "            roc_df.to_csv(save_name+'auc_roc.csv', index=False)\n",
    "            prc_df.to_csv(save_name+'auc_prc.csv', index=False)\n",
    "\n",
    "            # pickle.dump(models[classification_models[clf_idx]], open(f'{save_dir}/{classification_models[clf_idx]}_model.pkl', 'wb'))\n",
    "            # pd.DataFrame([metrics[classification_models[clf_idx]]]).to_csv(f'{save_dir}/{classification_models[clf_idx]}_metrics.csv', index=False)\n",
    "            plot_metrics(roc_data, prc_data, save_fig=True, filename=save_name)\n",
    "            #########################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}